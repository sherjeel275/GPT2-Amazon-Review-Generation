{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Finetuning GPT-2 For Generating Product Reviews",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qK_lPvRlBdMB",
        "colab_type": "text"
      },
      "source": [
        "# Finetuning GPT-2 for generating product reviews\n",
        "\n",
        "This colab is for fine-tuning GPT-2 on an Amazon review dataset using the [Hugging Face Transformer](https://github.com/huggingface/transformers) library.\n",
        "\n",
        "**IMPORTANT: Set GPU as the Hardware Accelerator in `Runtime > Change runtime type` before running the Colab.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biV1z0koaDHT",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGscdaCtpmbV",
        "colab_type": "text"
      },
      "source": [
        "### HuggingFace Transfomers library install + library imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uicio9FLPv5j",
        "colab_type": "code",
        "outputId": "371ba0e8-8871-406a-ff3b-0743798fcae4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers\n",
        "!git checkout b90745c\n",
        "\n",
        "import os\n",
        "os.chdir('/content/transformers')\n",
        "\n",
        "!pip install .\n",
        "!pip install -r ./examples/requirements.txt\n",
        "\n",
        "os.chdir('/content/transformers/examples')\n",
        "\n",
        "!pip install dict_to_obj\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'transformers' already exists and is not an empty directory.\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "Processing /content/transformers\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.5.1) (1.18.2)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers==2.5.1) (0.5.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.5.1) (1.12.40)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.5.1) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.5.1) (2.21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.5.1) (4.38.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.5.1) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.5.1) (0.1.85)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.5.1) (0.0.41)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.5.1) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.40 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.5.1) (1.15.40)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.5.1) (0.3.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.5.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.5.1) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.5.1) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.5.1) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.5.1) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.5.1) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.5.1) (1.12.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.40->boto3->transformers==2.5.1) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.40->boto3->transformers==2.5.1) (0.15.2)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-2.5.1-cp36-none-any.whl size=498745 sha256=c5fba42282270fe275f023f2f2bb48c51b55a4b9767322a7d49e4406322dc74c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9g7te0ly/wheels/23/19/dd/2561a4e47240cf6b307729d58e56f8077dd0c698f5992216cf\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Found existing installation: transformers 2.5.1\n",
            "    Uninstalling transformers-2.5.1:\n",
            "      Successfully uninstalled transformers-2.5.1\n",
            "Successfully installed transformers-2.5.1\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (from -r ./examples/requirements.txt (line 1)) (2.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (from -r ./examples/requirements.txt (line 2)) (2.2.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r ./examples/requirements.txt (line 3)) (0.22.2.post1)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (from -r ./examples/requirements.txt (line 4)) (0.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r ./examples/requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r ./examples/requirements.txt (line 1)) (1.18.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r ./examples/requirements.txt (line 1)) (3.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (0.9.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.6.0.post3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.28.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (46.1.3)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (0.34.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (2.21.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (3.2.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.7.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r ./examples/requirements.txt (line 3)) (0.14.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r ./examples/requirements.txt (line 3)) (1.4.1)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval->-r ./examples/requirements.txt (line 4)) (2.3.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r ./examples/requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r ./examples/requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r ./examples/requirements.txt (line 4)) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r ./examples/requirements.txt (line 4)) (2.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r ./examples/requirements.txt (line 4)) (1.0.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r ./examples/requirements.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (0.4.8)\n",
            "Requirement already satisfied: dict_to_obj in /usr/local/lib/python3.6/dist-packages (0.0.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weh0BoPfk1zc",
        "colab_type": "code",
        "outputId": "b69bc2ac-6964-4a27-85df-e99e7aea2a6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "import run_language_modeling\n",
        "import run_generation\n",
        "from dict_to_obj import DictToObj\n",
        "import collections\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX2LEl-uwMXp",
        "colab_type": "text"
      },
      "source": [
        "### Mount Google Drive\n",
        "Use **UPenn** Google Drive because it has unlimited storage space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQ6FFHiMMP0V",
        "colab_type": "code",
        "outputId": "231b8d89-1885-4a21-d938-7ab2c4ad6af2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEuEqWOhwn-a",
        "colab_type": "text"
      },
      "source": [
        "### (Optional) Obtain training, testing, and validation datasets for Amazon reviews.\n",
        "Training dataset should be roughly 20x larger than testing and dataset size should be between 5 and 100 MB.\n",
        "\n",
        "**This code will not run on colab unless you wget and unzip the Amazon review datasets from https://nijianmo.github.io/amazon/index.html and configure the directories properly.**\n",
        "\n",
        "Running this code will automatically generate the three datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GzNXh7ap_R3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import json\n",
        "\n",
        "# train_fout = open('reviews_train.txt', 'w')\n",
        "# train_link_fout = open('reviews_train_link.txt', 'w')\n",
        "# test_fout = open('reviews_test.txt', 'w')\n",
        "# test_link_fout = open('reviews_test_link.txt', 'w')\n",
        "# valid_fout = open('reviews_valid.txt', 'w')\n",
        "# valid_link_fout = open('reviews_valid_link.txt', 'w')\n",
        "# files_to_read = [\"All_Beauty_5.json\", \n",
        "#                  \"Books_5.json\", \n",
        "#                  \"Clothing_Shoes_and_Jewelry_5.json\", \n",
        "#                  \"Digital_Music_5.json\", \n",
        "#                  \"Electronics_5.json\", \n",
        "#                  \"Movies_and_TV_5.json\", \n",
        "#                  \"Toys_and_Games_5.json\", \n",
        "#                  \"Video_Games_5.json\"]\n",
        "\n",
        "# for index in range(len(files_to_read)):\n",
        "#     train_skips_per_review = 401\n",
        "#     test_skips_per_review = 8209\n",
        "#     valid_skips_per_review = 8017\n",
        "#     if index == 0:\n",
        "#         train_skips_per_review = 109\n",
        "#         test_skips_per_review = 2063\n",
        "#         valid_skips_per_review = 1901\n",
        "#     elif index == 1:\n",
        "#         train_skips_per_review = 2069\n",
        "#         test_skips_per_review = 42797\n",
        "#         valid_skips_per_review = 39733\n",
        "#     elif index == 3:\n",
        "#         train_skips_per_review = 223\n",
        "#         test_skips_per_review = 4099\n",
        "#         valid_skips_per_review = 4019\n",
        "#     elif index == 7:\n",
        "#         train_skips_per_review = 389\n",
        "#         test_skips_per_review = 8243\n",
        "#         valid_skips_per_review = 8069\n",
        "#     fin = open(files_to_read[index], 'r')\n",
        "#     raw_line = fin.readline()\n",
        "#     count = 0\n",
        "#     while len(raw_line) > 0:\n",
        "#         count += 1\n",
        "#         review_json = json.loads(raw_line)\n",
        "#         if \"reviewText\" in review_json:\n",
        "#             review_text = review_json[\"reviewText\"] # extract review text\n",
        "#             cleaned_review = review_text.replace('\\n', ' ') # remove newlines\n",
        "            \n",
        "#             if len(cleaned_review) > 800 and count % train_skips_per_review == 0 and count % test_skips_per_review != 0 and count % valid_skips_per_review != 0:\n",
        "#                 train_fout.write(cleaned_review + '\\n')\n",
        "#                 train_link_fout.write(cleaned_review + '\\n' + \"https://www.amazon.com/dp/\" + review_json[\"asin\"] + '\\n')\n",
        "            \n",
        "#             if len(cleaned_review) > 800 and count % train_skips_per_review != 0 and count % test_skips_per_review == 0 and count % valid_skips_per_review != 0:\n",
        "#                 test_fout.write(cleaned_review + '\\n')\n",
        "#                 test_link_fout.write(cleaned_review + '\\n' + \"https://www.amazon.com/dp/\" + review_json[\"asin\"] + '\\n')\n",
        "            \n",
        "#             if len(cleaned_review) > 800 and count % train_skips_per_review != 0 and count % test_skips_per_review != 0 and count % valid_skips_per_review == 0:\n",
        "#                 valid_fout.write(cleaned_review + '\\n')\n",
        "#                 valid_link_fout.write(cleaned_review + '\\n' + \"https://www.amazon.com/dp/\" + review_json[\"asin\"] + '\\n')\n",
        "#         raw_line = fin.readline()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AitVpu_-T6V4",
        "colab_type": "text"
      },
      "source": [
        "### Download Amazon review data.\n",
        "The resulting datasets from the above script can be downloaded to Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTX2cfaTUcS5",
        "colab_type": "code",
        "outputId": "b6d63cab-5aed-4028-e1fd-76c0c9a14ec7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Download the train, test, and validation datasets.\n",
        "!wget -nc -O /content/review_test.txt https://dl.dropbox.com/s/rp74kt5uhsc57jz/reviews_test.txt?dl=1\n",
        "!wget -nc -O /content/review_valid.txt https://dl.dropbox.com/s/i8ym3hfwkd3zxuw/reviews_valid.txt?dl=1\n",
        "!wget -nc -O /content/review_train.txt https://dl.dropbox.com/s/zqqfpx8pn6e9hd6/reviews_train.txt?dl=1\n",
        "!wget -nc -O /content/review_test_link.txt https://dl.dropbox.com/s/khqxx4kw7hyrk2g/reviews_test_link.txt?dl=1\n",
        "!wget -nc -O /content/review_valid_link.txt https://dl.dropbox.com/s/csmlwkkisozt3or/reviews_valid_link.txt?dl=1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File ‘/content/review_test.txt’ already there; not retrieving.\n",
            "File ‘/content/review_valid.txt’ already there; not retrieving.\n",
            "File ‘/content/review_train.txt’ already there; not retrieving.\n",
            "File ‘/content/review_test_link.txt’ already there; not retrieving.\n",
            "File ‘/content/review_valid_link.txt’ already there; not retrieving.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VzV8iTrphJl",
        "colab_type": "text"
      },
      "source": [
        "## Finetune and Eval\n",
        "[run_language_modeling.py](https://github.com/huggingface/transformers/blob/master/examples/run_language_modeling.py) will train and evaluate the language model.\n",
        "\n",
        "Call the script directly from the command line in order to launch training; also we will use functions from this script to conduct evaluation and generate samples at inference time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOzFhwDSqOg3",
        "colab_type": "text"
      },
      "source": [
        "### Launch fine-tuninng\n",
        "We will be calling `run_language_modeling.py` from the command line to launch fine-tuning, **Running fine-tuning may take several hours.** Every `save_steps` steps, a checkpoint is saved to disk. The checkpoint contains all the learned weights for your model, and you can  always reload the model from a saved checkpoint, even if your Colab has crashed.\n",
        "\n",
        "Below is an explanation of some of the arguments you might want to modify in the command below. \n",
        "\n",
        "* `--line_by_line`: Add `--line_by_line` if distinct lines of the text should be treated as distinct training examples. For example, if your dataset contains one story/tweet/article per line, this should be set.\n",
        "* `--num_train_epochs`: The number of times to iterate over the train set. Increasing the number of epochs may result in better performance, but making this number too high will cause the model to overfit on the train set.\n",
        "* `--block_size`: Your training text is truncated into blocks of this length. At test time, you will only want to generate sequences that are at most this length.\n",
        "* `--gradient_accumulation_steps`: Update the model weights every this many steps. You shold set this to >1 when the batch size is very small to improve training stability.\n",
        "* `--output_dir`: This is the where checkpoints will get saved. When you finetune on your own dataset, you should change this path. We recommend saving checkpoints to your Google Drive (`/content/drive/My Drive/`) so you can access them even if the Colab session dies.\n",
        "* `--model_name_or_path` The path to the model weights to use when starting fine-tuning. You can set this to `gpt2-medium` to initialize with GPT-2's 355 million parameter model, or `gpt2` to initialize with their smaller 124 million parameter model. You can also set this to one of your own checkpoints to restart your training job if it crashes.\n",
        "\n",
        "**I am getting out-of memory errors. What do I do?**\n",
        "\n",
        "The number of trainable paramters in the model is a function of the `block_size` and the `batch_size`. If you are getting out-of-memory errors, then try drecreasing these value.\n",
        "\n",
        "**Oh no! My computer went to sleep and the Colab disconnected.**\n",
        "\n",
        "The train job might have still completed. Check the `output_dir` in your Google Drive to see if checkpoint files have been created there.\n",
        "\n",
        "**Training is taking foreverrrrrr.**\n",
        "\n",
        "Try decreasing `num_train_epochs` or changing `model_name_or_path` to `gpt2` instead of `gpt2-medium`.\n",
        "If your evaluation set is very large, you might also want to remove the `evaluate_during_training` flag or increase `logging_steps`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C33GutF1QVEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python run_language_modeling.py \\\n",
        "    --output_dir='/content/drive/My Drive/finetuned_models/amazon_reviews' \\\n",
        "    --model_type=gpt2 \\\n",
        "    --model_name_or_path=gpt2-medium \\\n",
        "    --save_total_limit=5 \\\n",
        "    --num_train_epochs=4.0 \\\n",
        "    --do_train \\\n",
        "    --evaluate_during_training \\\n",
        "    --logging_steps=500 \\\n",
        "    --line_by_line \\\n",
        "    --save_steps=500 \\\n",
        "    --train_data_file=/content/review_train.txt \\\n",
        "    --do_eval \\\n",
        "    --eval_data_file=/content/review_valid.txt \\\n",
        "    --per_gpu_train_batch_size=2 \\\n",
        "    --per_gpu_eval_batch_size=2 \\\n",
        "    --block_size=128 \\\n",
        "    --gradient_accumulation_steps=5 \\\n",
        "    --overwrite_output_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9mAiosB2wBm",
        "colab_type": "text"
      },
      "source": [
        "### Compute perplexity of a dataset.\n",
        "This section shows how to compute perplexity of a dataset according to either the pre-trained or your fine-tuned language model. While this is possible to do by calling `run_language_modeling.py` on the command-line as above, we'll instead call the Python functions directly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiqSgGhtcDNd",
        "colab_type": "text"
      },
      "source": [
        "#### Look at what checkpoints are available\n",
        "Run `ls` to look at what checkpoints saved been saved. You'll want to set `CHECKPOINT_PATH` below to one of these in order to evaluate the model weights saved in that checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk_qHytBIETo",
        "colab_type": "code",
        "outputId": "4ea4b4ab-cf75-4c53-b629-643c7024267e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!ls '/content/drive/My Drive/finetuned_models/amazon_reviews'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint-1000  checkpoint-500    pytorch_model.bin\t    vocab.json\n",
            "checkpoint-1500  config.json\t   special_tokens_map.json\n",
            "checkpoint-2000  eval_results.txt  tokenizer_config.json\n",
            "checkpoint-2500  merges.txt\t   training_args.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRFwV1Ry3Evk",
        "colab_type": "text"
      },
      "source": [
        "#### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc2VCFBG3pFf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_model(args):\n",
        "  \"\"\"Creates a model and loads in weights for it.\"\"\"\n",
        "  config_class, model_class, _ = run_language_modeling.MODEL_CLASSES[args.model_type] \n",
        "  config = config_class.from_pretrained(args.model_name_or_path, cache_dir=None)\n",
        "\n",
        "  model = model_class.from_pretrained(\n",
        "      args.model_name_or_path,\n",
        "      from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
        "      config=config,\n",
        "      cache_dir=None,\n",
        "  )\n",
        "  model.to(args.device)\n",
        "  return model\n",
        "\n",
        "def set_seed(seed):\n",
        "  \"\"\"Set the random seed.\"\"\"\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "def do_perplexity_eval(args, model, data_file_path):\n",
        "  \"\"\"Computes the perplexity of the text in data_file_path according to the provided model.\"\"\"\n",
        "  set_seed(args.seed)\n",
        "\n",
        "  args.eval_data_file=data_file_path\n",
        "\n",
        "  _, _, tokenizer_class = run_language_modeling.MODEL_CLASSES[args.model_type]\n",
        "  tokenizer = tokenizer_class.from_pretrained(args.model_name_or_path, cache_dir=None)\n",
        "\n",
        "  args.block_size = min(args.block_size, tokenizer.max_len)\n",
        "\n",
        "  result = run_language_modeling.evaluate(args, model, tokenizer, prefix=\"\")\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kClE2Px-j9bb",
        "colab_type": "text"
      },
      "source": [
        "#### Compute it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERCKSncEBYgJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set this to the checkpoint you want to evalute, or to \"gpt2-medium\" to\n",
        "# evaluate the pre-trained model without finetuning.\n",
        "CHECKPOINT_PATH = '/content/drive/My Drive/finetuned_models/amazon_reviews/checkpoint-2500'\n",
        "# CHECKPOINT_PATH = \"gpt2-medium\"\n",
        "\n",
        "# Set this to the list of text files you want to evaluate the perplexity of.\n",
        "DATA_PATHS = [\"/content/review_valid.txt\",\n",
        "              \"/content/review_test.txt\"]\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "print(\"Running on device: \", device)\n",
        "\n",
        "args = collections.defaultdict(\n",
        "  model_name_or_path=CHECKPOINT_PATH,\n",
        "  output_dir=CHECKPOINT_PATH,\n",
        "  block_size = 128,\n",
        "  local_rank=-1,\n",
        "  eval_batch_size=2,\n",
        "  per_gpu_eval_batch_size=2,\n",
        "  n_gpu=n_gpu,\n",
        "  mlm=False,\n",
        "  device=device,\n",
        "  line_by_line=False,\n",
        "  overwrite_cache=None,\n",
        "  model_type='gpt2',\n",
        "  seed=42,\n",
        ")\n",
        "args = DictToObj(args)\n",
        "\n",
        "model = load_model(args)\n",
        "\n",
        "for data_path in DATA_PATHS:\n",
        "  eval_results = do_perplexity_eval(args, model, data_path)\n",
        "  perplexity = eval_results['perplexity']\n",
        "  print('{} is the perplexity of {} according to {}'.format(\n",
        "      perplexity, data_path, CHECKPOINT_PATH))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5o7v2hmhMTO",
        "colab_type": "text"
      },
      "source": [
        "### Generate samples\n",
        "The following code generates 5 text samples of length 800 characters for each review in the test dataset that are continuations of its first 140 characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcvySe_wrCWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_samples(args, model, prompt_text):\n",
        "  \"\"\"Generating sampling for the provided prompt using the provided model.\"\"\"\n",
        "  set_seed(args.seed)\n",
        "\n",
        "  _, _, tokenizer_class = run_language_modeling.MODEL_CLASSES[args.model_type]\n",
        "  tokenizer = tokenizer_class.from_pretrained(args.model_name_or_path, cache_dir=None)\n",
        "\n",
        "  requires_preprocessing = args.model_type in run_generation.PREPROCESSING_FUNCTIONS.keys()\n",
        "  encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n",
        "  encoded_prompt = encoded_prompt.to(args.device)\n",
        "\n",
        "  output_sequences = model.generate(\n",
        "      input_ids=encoded_prompt,\n",
        "      max_length=args.length + len(encoded_prompt[0]),\n",
        "      temperature=args.temperature,\n",
        "      top_k=args.k,\n",
        "      top_p=args.p,\n",
        "      repetition_penalty=args.repetition_penalty,\n",
        "      do_sample=True,\n",
        "      num_return_sequences=args.num_return_sequences,\n",
        "  )\n",
        "\n",
        "  # Remove the batch dimension when returning multiple sequences\n",
        "  if len(output_sequences.shape) > 2:\n",
        "    output_sequences.squeeze_()\n",
        "\n",
        "  generated_sequences = []\n",
        "\n",
        "  for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "    generated_sequence = generated_sequence.tolist()\n",
        "\n",
        "    # Decode text\n",
        "    text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "\n",
        "    # Remove all text after the stop token\n",
        "    text = text[: text.find(args.stop_token) if args.stop_token else None]\n",
        "\n",
        "    # Remove the excess text that was used for pre-processing\n",
        "    text = text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]\n",
        "\n",
        "    # Add the prompt at the beginning of the sequence.\n",
        "    total_sequence = prompt_text + text\n",
        "\n",
        "    generated_sequences.append(total_sequence)\n",
        "\n",
        "  return generated_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3LKo9VVjHw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set this to the checkpoint you want to use for generation, or to \"gpt2-medium\"\n",
        "# to generate with the pre-trained model without finetuning.\n",
        "CHECKPOINT_PATH = '/content/drive/My Drive/finetuned_models/amazon_reviews/checkpoint-500' #'gpt2-medium'\n",
        "\n",
        "files_to_read = ['/content/review_train.txt']\n",
        "# fout = open('/content/drive/My Drive/checkpoint-500-output.txt', 'w')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "print(\"Running on device: \", device)\n",
        "\n",
        "args = collections.defaultdict(\n",
        "  model_name_or_path=CHECKPOINT_PATH,\n",
        "  output_dir=CHECKPOINT_PATH,\n",
        "  n_gpu=n_gpu,\n",
        "  mlm=False,\n",
        "  device=device,\n",
        "  model_type='gpt2',\n",
        "  seed=42,\n",
        "  stop_token=None, # Set this if your dataset has a special word that indicates the end of a text.\n",
        "  temperature=1.0,  # temperature sampling. Set this to temperature=1.0 to not use temperature.\n",
        "  k=50,  # k for top-k sampling. Set this to k=0 to not use top-k.\n",
        "  p=1.0,  # p for nucleus sampling. Set this to p=1.0 to not use nucleus sampling.\n",
        "  repetition_penalty=None,\n",
        "  length=300,  # Number of tokens to generate.\n",
        "  num_return_sequences=1,  # Number of independently computed samples to generate.\n",
        ")\n",
        "args = DictToObj(args)\n",
        "\n",
        "model = load_model(args)\n",
        "\n",
        "for file_to_read in files_to_read:\n",
        "  fin = open(file_to_read, 'r')\n",
        "  review_text = fin.readline()\n",
        "  for i in range(0,85):\n",
        "    review_text = fin.readline()\n",
        "  count = 0\n",
        "  while len(review_text) > 0 and count < 500:\n",
        "    # review_link = fin.readline()\n",
        "    PROMPT = review_text[:70]\n",
        "    last_space = PROMPT.rfind(' ')\n",
        "    if last_space != -1:\n",
        "      PROMPT = PROMPT[:last_space]\n",
        "    sequences = generate_samples(args, model, PROMPT)\n",
        "    # print(review_link + '\\n')\n",
        "    for idx, sequence in enumerate(sequences):\n",
        "      # print('====== GENERATION {} ======'.format(idx))\n",
        "      full_review = sequence.replace('\\n', '')\n",
        "      if full_review.find('.') != -1 or full_review.find('!') != -1 or full_review.find('?') != -1:\n",
        "        last_period_index = min(800, len(full_review))\n",
        "        if last_period_index == 800:\n",
        "          while full_review[last_period_index] != '.' and full_review[last_period_index] != '?' and full_review[last_period_index] != '!':\n",
        "            last_period_index -= 1\n",
        "        print(full_review[:last_period_index + 1] + '\\n')\n",
        "    review_text = fin.readline()\n",
        "    count += 1"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}